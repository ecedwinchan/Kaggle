{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38abf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\"data/detecting-reversal-points-in-us-equities/train.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b23eda",
   "metadata": {},
   "source": [
    "- Feature Selection\n",
    "\n",
    "- Model Building\n",
    "\n",
    "- Evaluation Metric (in order of importance):\n",
    "    1. Macro F1-score â€” treats all swing classes equally, regardless of class imbalance.\n",
    "    2. Macro Balanced Accuracy\n",
    "    3. Matthews Correlation Coefficient (multi-class)\n",
    "    4. Inference runtime\n",
    "\n",
    "    This ensures that winning models capture swing structure across all four categories, not just the majority class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759a6fc8",
   "metadata": {},
   "source": [
    "#### Clean data + Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5760a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"train_id\"].unique() # [0,    1,    2, ..., 1929, 1930, 1931]\n",
    "train_data[\"ticker_id\"].unique() # [2, 3, 6, 1, 4, 5]\n",
    "train_data[\"class_label\"].unique() # [nan, 'HL', 'HH', 'LH', 'LL']\n",
    "\n",
    "# Non boolean features\n",
    "numerical_features = ['momentum', 'ratio', 'sm_momentum', 'sm_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942b6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"class_label\"] = train_data[\"class_label\"].fillna(\"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989de98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_data.select_dtypes(include=['object']).columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde844d7",
   "metadata": {},
   "source": [
    "#### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0165fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import compute_sample_weight\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "\n",
    "models = [\n",
    "    # RandomForestClassifier(class_weight='balanced'), # class_weight='balanced'\n",
    "\n",
    "    # LogisticRegression(\n",
    "    #     penalty='l1', \n",
    "    #     solver='liblinear',     \n",
    "    #     class_weight='balanced',\n",
    "    #     max_iter=1000\n",
    "    # ),\n",
    "    \n",
    "    xgb.XGBClassifier(),\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    all_scores = []\n",
    "    for id in sorted(list(train_data[\"ticker_id\"].unique())):\n",
    "        # so that each model starts fresh\n",
    "        tmp_model = copy(model)\n",
    "\n",
    "        focused_df = train_data[train_data[\"ticker_id\"] == id].sort_values(by=\"t\")\n",
    "        focused_df.set_index(\"t\", inplace=True)\n",
    "        focused_df.drop(columns=[\"train_id\", \"ticker_id\"], inplace=True)\n",
    "        X, y = focused_df.drop(columns=[\"class_label\"]), focused_df[\"class_label\"]\n",
    "        # remove duplicated columns\n",
    "        X = X.loc[:, ~X.T.duplicated()]\n",
    "\n",
    "        print(X.shape)\n",
    "\n",
    "        # Encode labels for XGBClassifier\n",
    "        if model.__class__.__name__ == \"XGBClassifier\":\n",
    "            le = LabelEncoder()\n",
    "            y = le.fit_transform(y.astype(str))\n",
    "            \n",
    "\n",
    "        # train-test split\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "        \n",
    "        if model.__class__.__name__ == \"XGBClassifier\":\n",
    "            sample_weights = compute_sample_weight(class_weight='balanced', y=train_y)\n",
    "            tmp_model.fit(train_X, train_y, sample_weight=sample_weights)\n",
    "\n",
    "            # Calculate learning curves\n",
    "            train_sizes, train_scores, test_scores = learning_curve(\n",
    "                estimator=tmp_model, X=X, y=y, scoring='f1_macro', cv=2,\n",
    "                train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "            )\n",
    "\n",
    "            # Calculate mean and standard deviation of scores\n",
    "            train_mean = np.mean(train_scores, axis=1)\n",
    "            train_std = np.std(train_scores, axis=1)\n",
    "            test_mean = np.mean(test_scores, axis=1)\n",
    "            test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "            # Plot learning curves\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(train_sizes, train_mean, color='blue', marker='o', label='Training accuracy')\n",
    "            plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n",
    "            plt.plot(train_sizes, test_mean, color='green', marker='+', label='Validation accuracy')\n",
    "            plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\n",
    "            plt.title('Learning Curves')\n",
    "            plt.xlabel('Training set size')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.grid()\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.show()\n",
    "\n",
    "        else:\n",
    "            tmp_model.fit(train_X, train_y)\n",
    "        \n",
    "        y_pred = tmp_model.predict(val_X)\n",
    "        score = f1_score(val_y, y_pred, average='macro')\n",
    "        all_scores.append(score)\n",
    "        print(f\"id: {id}, macro F1: {float(score):.4f}\")\n",
    "    print(f\"Overall macro F1: {float(np.mean(all_scores)):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7fbfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "focused_df = train_data[train_data[\"ticker_id\"] == 2].sort_values(by=\"t\")\n",
    "focused_df.set_index(\"t\", inplace=True)\n",
    "focused_df.drop(columns=[\"train_id\", \"ticker_id\"], inplace=True)\n",
    "X, y = focused_df.drop(columns=[\"class_label\"]), focused_df[\"class_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eda075",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_cleaned =  X.T.drop_duplicates().T\n",
    "print(X_cleaned.shape)\n",
    "cols_to_drop = [col for col in X_cleaned.columns if X_cleaned[col].nunique() == 1]\n",
    "X_cleaned = X_cleaned.drop(columns=cols_to_drop)\n",
    "print(X_cleaned.shape)\n",
    "X.columns.difference(X_cleaned.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc1e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff83f870",
   "metadata": {},
   "source": [
    "#### Features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dfa9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Global safe filtering (no leakage)\n",
    "def reduce_features_global(X):\n",
    "    # Remove near-zero variance\n",
    "    sel_var = VarianceThreshold(threshold=1e-5)\n",
    "    X_reduced = sel_var.fit_transform(X)\n",
    "    \n",
    "    # Optional: remove duplicates\n",
    "    # _, unique_idx = np.unique(X_reduced, axis=1, return_index=True)\n",
    "    # X_reduced = X_reduced[:, sorted(unique_idx)]\n",
    "    \n",
    "    return X_reduced, sel_var\n",
    "\n",
    "# full_X = train_data.drop(columns=[\"train_id\", \"ticker_id\", \"t\", \"class_label\"])\n",
    "# X_reduced, _ = reduce_features_global(full_X.values)\n",
    "# train_data_red = pd.concat([\n",
    "#     train_data[[\"ticker_id\", \"t\", \"class_label\"]].reset_index(drop=True),\n",
    "#     pd.DataFrame(X_reduced)\n",
    "# ], axis=1)\n",
    "\n",
    "# === 2. Per-ticker modeling ===\n",
    "for base_model in models:\n",
    "    all_scores = []\n",
    "    for id in sorted(train_data[\"ticker_id\"].unique()):\n",
    "        df = train_data[train_data[\"ticker_id\"] == id].sort_values(\"t\")\n",
    "        X, y = df.drop(columns=[\"ticker_id\", \"t\", \"class_label\"]), df[\"class_label\"]\n",
    "\n",
    "        X_reduced, _ = reduce_features_global(X.values)\n",
    "        print(len(X_reduced[0]))\n",
    "        \n",
    "        # TimeSeries CV\n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        fold_scores = []\n",
    "        for train_idx, val_idx in tscv.split(X):\n",
    "            model = clone(base_model)\n",
    "            X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "            \n",
    "            # Handle XGBoost label encoding\n",
    "            if isinstance(model, xgb.XGBClassifier):\n",
    "                le = LabelEncoder()\n",
    "                y_tr_enc = le.fit_transform(y_tr)\n",
    "                y_val_enc = le.transform(y_val)\n",
    "                model.fit(X_tr, y_tr_enc)\n",
    "                y_pred = le.inverse_transform(model.predict(X_val))\n",
    "            else:\n",
    "                model.fit(X_tr, y_tr)\n",
    "                y_pred = model.predict(X_val)\n",
    "                \n",
    "            fold_scores.append(f1_score(y_val, y_pred, average='macro'))\n",
    "        \n",
    "        all_scores.append(np.mean(fold_scores))\n",
    "    print(f\"{model.__class__.__name__}: {np.mean(all_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f6ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, VarianceThreshold\n",
    "import numpy as np\n",
    "\n",
    "selector_var = VarianceThreshold(threshold=0.01)  # Keep features with var > 0.01\n",
    "X_var = selector_var.fit_transform(train_X)\n",
    "feature_names_after_var = np.array(train_X.columns)[selector_var.get_support()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c21690",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_after_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1922d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dce1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a706cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1142d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 7):\n",
    "    df = train_data[train_data[\"ticker_id\"] == i]\n",
    "    print(f\"Ticker ID: {i}\")\n",
    "    print(df[\"class_label\"].value_counts(dropna=False), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724af10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f579b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"123\".replace(\"3\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7fb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "all_feature_names = train_data.columns.to_list()\n",
    "all_feature_names_dict = {}\n",
    "\n",
    "for feature_name in all_feature_names:\n",
    "    if len(feature_name) >= 2 and feature_name[-2] == '_' and feature_name[-1].isdigit():\n",
    "        feature_name = feature_name[:-2]\n",
    "    elif feature_name [-3:] == \"_10\":\n",
    "        feature_name = feature_name[:-3]\n",
    "    name = (\"_\").join(feature_name.split(\"_\")[:-1])\n",
    "\n",
    "    if name not in all_feature_names_dict:\n",
    "        print(feature_name)\n",
    "        all_feature_names_dict[name] = []\n",
    "\n",
    "# for i in list(all_feature_names_dict.keys()):\n",
    "#     print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
